# ***********************************************************************************************
# !/usr/bin/env python
# coding: utf-8
# ***********************************************************************************************
# Title: Forest Cover Type Prediction
# Course & Group : PGP in AI & ML - AIML2020 Cohort 4, Group 1
# Subject: Capstone Project - PCAMZC321
# Description: This is the python script that implements the core metods needed by ForestCover.py
#
# Lead Instructor:
# Mr. Satyaki Dasgupta
#
# Student Names :
## Chandresh Khaneja | 2020AIML062
## Saurabh Gupta     | 2020AIML065
## Sudheendran T L   | 2020AIML003
## Sudhir Valluri    | 2020AIML001
#
# ***********************************************************************************************
#
# Notes on this python script:
# 1) This file is generated by downloading the .py file of covtype.ipynb from Jupyter notebook
# 2) This file should be in the same path as the ForestCover.py file
#
# ***********************************************************************************************
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import warnings

from category_encoders import TargetEncoder
from imblearn.combine import SMOTETomek
from sklearn.decomposition import PCA, IncrementalPCA
from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import MinMaxScaler, StandardScaler

warnings.filterwarnings("ignore")

def optimizeDataFrameSize(df):

    for col in df:

        if ( df[col].dtype == 'int64' ) :

            inttype = 'int16'
            if ( df[col].max() <= 255 ):
                inttype = 'int8'
            df[col] = df[col].astype(inttype)

    return df

def createCatData(data, cols):
    df = data.copy()
    df['Wilderness_Area'] = (df.iloc[:, 10:14] == 1).idxmax(axis='columns')
    df['Soil_Type'] = (df.iloc[:, 14:54] == 1).idxmax(axis='columns')
    return df.reindex(columns = cols)

numcols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',
           'Horizontal_Distance_To_Roadways', 'Hillshade_9am',  'Hillshade_Noon', 'Hillshade_3pm',
           'Horizontal_Distance_To_Fire_Points']
catcols = ['Soil_Type', 'Wilderness_Area']
ycols = ['Cover_Type']
xcols = numcols+catcols
cols = xcols+ycols

def createTargetEncData(data, cols, target):
    df = data.copy()
    for col in cols: df[col] = TargetEncoder().fit_transform(df[col], df[target])
    return df

def removeLOFOutliers(x, y, target):
	xo = x.copy()
	yo = y.copy()
	xo['ClusterPred'] = LocalOutlierFactor().fit_predict(xo)
	print(xo['ClusterPred'].value_counts())

	o_rows = xo[ xo['ClusterPred'] == -1 ].index

	if ( len(o_rows) > 0 ):
		xo.drop(o_rows, inplace=True)
		xo.reset_index(drop=True, inplace=True)
		yo.drop(o_rows, inplace=True)
		yo.reset_index(drop=True, inplace=True)
		yo = pd.DataFrame(yo, columns=[target])

	xo.drop(['ClusterPred'], axis=1, inplace=True)
	return xo, yo, len(o_rows)

def performSMOTETomek(x,y, xols, ycols):
    smote = SMOTETomek( random_state = 42 )
    bx, by = smote.fit_resample(x, y)
    bx = pd.DataFrame(bx, columns=xcols)
    by = pd.DataFrame(by, columns=ycols)
    bx.reset_index(drop=True, inplace=True)
    by.reset_index(drop=True, inplace=True)
    # check for duplicated rows and remove barring first occurrence
    print(sum(bx.duplicated().values))
    d_rows = bx.loc[bx.duplicated()==True].index
    bx.drop(d_rows, inplace=True)
    bx.reset_index(drop=True, inplace=True)
    by.drop(d_rows, inplace=True)
    by.reset_index(drop=True, inplace=True)
    bx = pd.DataFrame(bx, columns=xcols)
    by = pd.DataFrame(by, columns=ycols)
    return bx, by

def createBinnedData(df, cols):
    binx = df.copy()
    for col in cols: binx[col] = pd.qcut(binx[col], 10, labels=False)
    return binx

def minMaxScaleData (df, cols):
    msdf = df.copy()
    msdf = MinMaxScaler().fit_transform(msdf)
    return pd.DataFrame(msdf, columns = cols)

def stdScaleData (df, cols):
    ssdf = df.copy()
    ssdf = StandardScaler().fit_transform(ssdf)
    return pd.DataFrame(ssdf, columns = cols)

def performPCA (x, cols):
    pcax = PCA(svd_solver='randomized', random_state=42)
    pcax.fit(x)
    print(pcax.components_)
    plt.plot(np.cumsum(pcax.explained_variance_ratio_))
    plt.xlabel('Number of Components')
    plt.ylabel('Cumulative Explained Variance')
    plt.title('PCA Scree Plot')
    plt.grid()
    plt.show()
    return pd.DataFrame({'PC1':pcax.components_[0],'PC2':pcax.components_[1], 'Feature':xcols})

def performIncrementalPCA(xdata):
    pca_final = IncrementalPCA(n_components=4)
    pcax = pca_final.fit_transform(xdata)
    return pd.DataFrame(pcax)

# end of file